# ä¼ è¾“é‡åˆ†ææŠ¥å‘Š

## é—®é¢˜æ¦‚è¿°

åœ¨ä½¿ç”¨ **LLaMA 3.2-1B** æ¨¡å‹ã€**1024 tokens** çš„æƒ…å†µä¸‹ï¼Œéƒ¨åˆ†æ“ä½œçš„ä¼ è¾“é‡å¼‚å¸¸å¤§ã€‚

## æ•°æ®åˆ†æ

### æ€»ä½“ç»Ÿè®¡
- **æ€» RPC è°ƒç”¨**: 98 æ¬¡
- **æ€»å‘é€æ•°æ®**: 3328.02 MB
- **æ€»æ¥æ”¶æ•°æ®**: 3656.50 MB
- **æ€»ä¼ è¾“é‡**: **6984.52 MB (~7GB)**
- **å¹³å‡ååé‡**: 162.43 MB/s

### ä¸»è¦æ“ä½œä¼ è¾“é‡

| æ“ä½œç±»å‹ | å‘é€é‡ (KB) | æ¥æ”¶é‡ (KB) | æ€»é‡ (MB) | å æ¯” |
|---------|------------|------------|----------|------|
| **Matmul** | 16384 æˆ– 139264 | 131072 æˆ– 8192 | **128-144 MB/æ¬¡** | **æœ€å¤§** |
| BatchLinear | 8192 æˆ– 32768 | 8192-65536 | 8-72 MB/æ¬¡ | ä¸­ç­‰ |
| Embedding | 8 | 8192 | 8 MB/æ¬¡ | å° |
| LMHead | 8 | 501 | 0.5 MB/æ¬¡ | æœ€å° |

## ğŸ”´ æ ¸å¿ƒé—®é¢˜ï¼šMatmul ä¼ è¾“é‡è¿‡å¤§

### é—®é¢˜è¯¦æƒ…

ä»æ—¥å¿—ä¸­å¯ä»¥çœ‹åˆ°ï¼Œ**Matmul** æ“ä½œæœ‰ä¸¤ç§æ¨¡å¼ï¼š

#### æ¨¡å¼ 1ï¼šQ @ K^Tï¼ˆAttention Scoresï¼‰
```
ID 4: Matmul
  Sent:     16384.08 KB  (16 MB)   â† Q: [1, 32, 1024, 64]
  Received: 131072.05 KB (128 MB)  â† Scores: [1, 32, 1024, 1024]
  Total:    144 MB
```

#### æ¨¡å¼ 2ï¼šScores @ Vï¼ˆAttention Outputï¼‰
```
ID 5: Matmul
  Sent:     139264.08 KB (136 MB)  â† Scores: [1, 32, 1024, 1024] + V: [1, 32, 1024, 64]
  Received: 8192.05 KB   (8 MB)    â† Output: [1, 32, 1024, 64]
  Total:    144 MB
```

### ğŸ¯ æ ¹æœ¬åŸå› 

**Attention Scores çŸ©é˜µè¿‡å¤§ï¼**

```python
# Attention è®¡ç®—æµç¨‹
Q: [batch=1, heads=32, seq_len=1024, head_dim=64]  # 8 MB
K: [batch=1, heads=32, seq_len=1024, head_dim=64]  # 8 MB

# é—®é¢˜åœ¨è¿™é‡Œ â†“
Scores = Q @ K^T  # [1, 32, 1024, 1024]  â† 128 MBï¼ï¼ï¼

# ç„¶å
Output = Scores @ V  # [1, 32, 1024, 64]  # 8 MB
```

### æ•°å­¦è®¡ç®—

**Attention Scores å¤§å°**ï¼š
```
Shape: [batch, num_heads, seq_len, seq_len]
     = [1, 32, 1024, 1024]
     
Size = 1 Ã— 32 Ã— 1024 Ã— 1024 Ã— 4 bytes (float32)
     = 134,217,728 bytes
     = 128 MB
```

**æ¯å±‚ Attention çš„ä¼ è¾“é‡**ï¼š
- Q @ K^T: å‘é€ 16MBï¼Œæ¥æ”¶ 128MB = **144 MB**
- Scores @ V: å‘é€ 136MBï¼Œæ¥æ”¶ 8MB = **144 MB**
- **æ¯å±‚æ€»è®¡**: **288 MB**

**16 å±‚ Decoder çš„æ€»ä¼ è¾“é‡**ï¼š
```
16 layers Ã— 288 MB/layer = 4608 MB â‰ˆ 4.5 GB
```

è¿™ä¸æ—¥å¿—ä¸­çš„æ€»ä¼ è¾“é‡ **~7GB** åŸºæœ¬å»åˆï¼ˆè¿˜åŒ…æ‹¬ BatchLinearã€Embedding ç­‰ï¼‰ã€‚

## ğŸ” è¯¦ç»†åˆ†è§£

### LLaMA 3.2-1B æ¨¡å‹é…ç½®
```python
num_layers = 16
hidden_size = 2048
num_heads = 32
head_dim = 64
seq_len = 1024  # Prefill length
```

### æ¯å±‚çš„ä¼ è¾“é‡è®¡ç®—

#### 1. Attention éƒ¨åˆ†
```
Q/K/V Projections (BatchLinear):
  Input:  [1, 1024, 2048] = 8 MB
  Output: [1, 1024, 2048] Ã— 3 = 24 MB
  Total:  32 MB

Q @ K^T (Matmul):
  Q:      [1, 32, 1024, 64] = 8 MB
  K^T:    [1, 32, 64, 1024] = 8 MB (å·²åŒ…å«åœ¨ Q ä¸­)
  Scores: [1, 32, 1024, 1024] = 128 MB  â† é—®é¢˜ï¼
  Total:  144 MB

Scores @ V (Matmul):
  Scores: [1, 32, 1024, 1024] = 128 MB
  V:      [1, 32, 1024, 64] = 8 MB
  Output: [1, 32, 1024, 64] = 8 MB
  Total:  144 MB

O Projection (BatchLinear):
  Input:  [1, 1024, 2048] = 8 MB
  Output: [1, 1024, 2048] = 8 MB
  Total:  16 MB

Attention å°è®¡: 32 + 144 + 144 + 16 = 336 MB
```

#### 2. MLP éƒ¨åˆ†
```
Gate/Up Projections (BatchLinear):
  Input:  [1, 1024, 2048] = 8 MB
  Output: [1, 1024, 8192] Ã— 2 = 64 MB
  Total:  72 MB

Down Projection (BatchLinear):
  Input:  [1, 1024, 8192] = 32 MB
  Output: [1, 1024, 2048] = 8 MB
  Total:  40 MB

MLP å°è®¡: 72 + 40 = 112 MB
```

#### 3. æ¯å±‚æ€»è®¡
```
Attention: 336 MB
MLP:       112 MB
Total:     448 MB/layer
```

#### 4. å…¨æ¨¡å‹æ€»è®¡
```
Embedding:  8 MB
16 Layers:  448 Ã— 16 = 7168 MB â‰ˆ 7 GB
LM Head:    0.5 MB
Total:      ~7.2 GB
```

**ä¸æ—¥å¿—å»åˆï¼** âœ…

## ğŸš¨ ä¸ºä»€ä¹ˆä¼ è¾“é‡è¿™ä¹ˆå¤§ï¼Ÿ

### åŸå›  1ï¼šAttention Scores æ˜¯äºŒæ¬¡å¤æ‚åº¦
```
Scores = Q @ K^T
Shape: [batch, heads, seq_len, seq_len]
                              â†‘       â†‘
                              è¿™ä¸¤ä¸ªç»´åº¦ç›¸ä¹˜ï¼

å½“ seq_len = 1024 æ—¶ï¼š
Size âˆ seq_lenÂ²
    = 1024Â² = 1,048,576 ä¸ªå…ƒç´ /head
    Ã— 32 heads
    Ã— 4 bytes
    = 128 MB
```

### åŸå›  2ï¼šå½“å‰æ¶æ„éœ€è¦ä¼ è¾“ä¸­é—´ç»“æœ
```
TEE ç«¯                          GPU ç«¯
  â†“                               â†“
Q, K, V â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Linear
  â†“                               â†“
Reshape, RoPE                     â†“
  â†“                               â†“
Q, K â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Matmul (Q @ K^T)
  â†“                               â†“
Scores (128MB!) <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â†“
  â†“                               â†“
Softmax                           â†“
  â†“                               â†“
Scores (128MB!) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Matmul (Scores @ V)
  â†“                               â†“
Output <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â†“
```

**æ¯æ¬¡éƒ½è¦ä¼ è¾“ 128MB çš„ Scoresï¼**

### åŸå›  3ï¼šFloat32 ç²¾åº¦
```
å½“å‰ä½¿ç”¨ float32 (4 bytes)
å¦‚æœä½¿ç”¨ bfloat16 (2 bytes)ï¼Œå¯ä»¥å‡åŠï¼š
  128 MB â†’ 64 MB
```

## ğŸ’¡ ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ bfloat16ï¼ˆç«‹å³å¯è¡Œï¼‰âœ…
```python
# ä¿®æ”¹ wire_dtype
wire_dtype = "bfloat16"  # ä» float32 æ”¹ä¸º bfloat16

é¢„æœŸæ•ˆæœï¼š
  ä¼ è¾“é‡å‡åŠï¼š7 GB â†’ 3.5 GB
  æ€§èƒ½æå‡ï¼š~2x
```

### æ–¹æ¡ˆ 2ï¼šFused Attentionï¼ˆæ¨èï¼‰â­
```python
# å°†æ•´ä¸ª Attention æ”¾åœ¨ GPU ç«¯æ‰§è¡Œ
def fused_attention_gpu(Q, K, V):
    # åœ¨ GPU ç«¯å®Œæˆæ‰€æœ‰æ“ä½œï¼Œä¸ä¼ è¾“ä¸­é—´ç»“æœ
    scores = Q @ K.T
    scores = softmax(scores)
    output = scores @ V
    return output

ä¼˜åŠ¿ï¼š
  - ä¸ä¼ è¾“ Scores (128MB Ã— 2)
  - æ¯å±‚èŠ‚çœ 256 MB
  - 16 å±‚èŠ‚çœ 4 GB
  - æ€»ä¼ è¾“é‡ï¼š7 GB â†’ 3 GB
```

### æ–¹æ¡ˆ 3ï¼šFlash Attentionï¼ˆæœ€ä¼˜ï¼‰ğŸš€
```python
# ä½¿ç”¨ Flash Attention ç®—æ³•
# ä¸éœ€è¦æ˜¾å¼è®¡ç®—å®Œæ•´çš„ Scores çŸ©é˜µ

ä¼˜åŠ¿ï¼š
  - å†…å­˜å ç”¨ O(N) è€Œä¸æ˜¯ O(NÂ²)
  - ä¸ä¼ è¾“ Scores
  - é€Ÿåº¦æ›´å¿«
  - æ€»ä¼ è¾“é‡ï¼š7 GB â†’ 2.5 GB
```

### æ–¹æ¡ˆ 4ï¼šåˆ†å—ä¼ è¾“ï¼ˆè¾…åŠ©ä¼˜åŒ–ï¼‰
```python
# å¯¹äºå¤§çŸ©é˜µï¼Œåˆ†å—ä¼ è¾“
chunk_size = 256
for i in range(0, seq_len, chunk_size):
    chunk = scores[:, :, i:i+chunk_size, :]
    # å¤„ç† chunk

ä¼˜åŠ¿ï¼š
  - é™ä½å³°å€¼å†…å­˜
  - å¯ä»¥ä¸å…±äº«å†…å­˜ç»“åˆ
```

## ğŸ“Š ä¼˜åŒ–æ•ˆæœé¢„æµ‹

| æ–¹æ¡ˆ | ä¼ è¾“é‡ | å»¶è¿Ÿ | å®ç°éš¾åº¦ |
|-----|-------|------|---------|
| å½“å‰ (float32) | 7.0 GB | åŸºå‡† | - |
| bfloat16 | 3.5 GB | -50% | â­ ç®€å• |
| Fused Attention | 3.0 GB | -60% | â­â­ ä¸­ç­‰ |
| Flash Attention | 2.5 GB | -70% | â­â­â­ å›°éš¾ |
| ç»„åˆä¼˜åŒ– | 1.5 GB | -80% | â­â­â­â­ å¾ˆéš¾ |

## ğŸ¯ ç«‹å³è¡ŒåŠ¨å»ºè®®

### çŸ­æœŸï¼ˆ1-2å¤©ï¼‰
1. **å¯ç”¨ bfloat16**
   ```python
   # åœ¨ init() ä¸­è®¾ç½®
   wire_dtype = "bfloat16"
   ```
   - é¢„æœŸæ•ˆæœï¼šä¼ è¾“é‡å‡åŠ
   - é£é™©ï¼šä½ï¼ˆå·²æœ‰ä»£ç æ”¯æŒï¼‰

2. **éªŒè¯å…±äº«å†…å­˜ä¼˜åŒ–**
   - å½“å‰ 10MB é˜ˆå€¼å¯¹ Scores (128MB) æ— æ•ˆ
   - è€ƒè™‘æé«˜é˜ˆå€¼æˆ–ä½¿ç”¨å‹ç¼©

### ä¸­æœŸï¼ˆ1-2å‘¨ï¼‰
3. **å®ç° Fused Attention**
   ```python
   def handle_fused_attention(self, request):
       Q, K, V = request["Q"], request["K"], request["V"]
       # åœ¨ GPU ç«¯å®Œæˆæ‰€æœ‰ Attention è®¡ç®—
       output = fused_attention(Q, K, V)
       return output
   ```

4. **ä¼˜åŒ– BatchLinear**
   - åˆå¹¶å¤šä¸ª Linear æ“ä½œ
   - å‡å°‘å¾€è¿”æ¬¡æ•°

### é•¿æœŸï¼ˆ1ä¸ªæœˆ+ï¼‰
5. **é›†æˆ Flash Attention**
   - ä½¿ç”¨ `flash-attn` åº“
   - éœ€è¦ä¿®æ”¹æ¨¡å‹ç»“æ„

6. **åŠ¨æ€é˜ˆå€¼è°ƒæ•´**
   - æ ¹æ®æ•°æ®å¤§å°è‡ªåŠ¨é€‰æ‹©ä¼ è¾“æ–¹å¼
   - è‡ªé€‚åº”å‹ç¼©

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

### å½“å‰æ€§èƒ½
```
Prefill (1024 tokens):
  Total Time: ~43 seconds (98 calls Ã— 438ms/call)
  Throughput: 162 MB/s
  Bottleneck: Matmul (Scores ä¼ è¾“)
```

### ä¼˜åŒ–åé¢„æœŸ
```
ä½¿ç”¨ bfloat16 + Fused Attention:
  Total Time: ~15 seconds (ä¼°è®¡)
  Throughput: 400+ MB/s
  æ”¹è¿›: 3x åŠ é€Ÿ
```

## ğŸ”§ ä»£ç ä¿®æ”¹ç¤ºä¾‹

### 1. å¯ç”¨ bfloat16
```python
# tee_runner_optimized.py
def init(self) -> Dict:
    meta = {
        "wire_dtype": "bfloat16",  # â† æ”¹è¿™é‡Œ
        "max_chunks": 10,
    }
    # ...
```

### 2. å®ç° Fused Attentionï¼ˆæœåŠ¡ç«¯ï¼‰
```python
# server_optimized.py
@torch.no_grad()
def fused_attention(self, layer_idx: int, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
    """Fused Attention - åœ¨ GPU ç«¯å®Œæˆæ‰€æœ‰è®¡ç®—"""
    # Q, K, V: [batch, heads, seq_len, head_dim]
    
    # 1. Q @ K^T
    scores = torch.matmul(Q, K.transpose(-2, -1))
    
    # 2. Scale
    scores = scores * (self.head_dim ** -0.5)
    
    # 3. Softmax
    scores = F.softmax(scores, dim=-1)
    
    # 4. Scores @ V
    output = torch.matmul(scores, V)
    
    return output  # åªè¿”å›æœ€ç»ˆç»“æœï¼Œä¸ä¼ è¾“ä¸­é—´ Scores
```

### 3. å®¢æˆ·ç«¯è°ƒç”¨
```python
# tee_runner_optimized.py
def attention(self, layer_idx: int, hidden_states: torch.Tensor, position_ids: torch.Tensor) -> torch.Tensor:
    # 1. QKV projections (GPU)
    qkv = self.gpu.batch_linear(layer_idx, ["q_proj", "k_proj", "v_proj"], hidden_states)
    Q, K, V = qkv
    
    # 2. Reshape (TEE)
    Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
    K = K.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
    V = V.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
    
    # 3. RoPE (TEE)
    cos, sin = self.rotary_emb(V, position_ids)
    Q, K = apply_rotary_pos_emb(Q, K, cos, sin)
    K = repeat_kv(K, self.num_key_value_groups)
    V = repeat_kv(V, self.num_key_value_groups)
    
    # 4. Fused Attention (GPU) â† æ–°å¢
    attn_output = self.gpu.fused_attention(layer_idx, Q, K, V)
    
    # 5. Reshape (TEE)
    attn_output = attn_output.transpose(1, 2).contiguous()
    attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)
    
    # 6. O projection (GPU)
    attn_output = self.gpu.batch_linear(layer_idx, ["o_proj"], attn_output)[0]
    
    return attn_output
```

## æ€»ç»“

### é—®é¢˜æ ¹æº
**Attention Scores çŸ©é˜µçš„äºŒæ¬¡å¤æ‚åº¦**å¯¼è‡´ä¼ è¾“é‡å·¨å¤§ï¼š
- Scores: [1, 32, 1024, 1024] = **128 MB**
- æ¯å±‚ä¼ è¾“ 2 æ¬¡ = **256 MB/layer**
- 16 å±‚ = **4 GB** (å æ€»ä¼ è¾“é‡çš„ 57%)

### è§£å†³æ–¹æ¡ˆä¼˜å…ˆçº§
1. âœ… **ç«‹å³**: å¯ç”¨ bfloat16ï¼ˆå‡åŠä¼ è¾“é‡ï¼‰
2. â­ **æ¨è**: å®ç° Fused Attentionï¼ˆå‡å°‘ 60% ä¼ è¾“é‡ï¼‰
3. ğŸš€ **æœ€ä¼˜**: é›†æˆ Flash Attentionï¼ˆå‡å°‘ 70% ä¼ è¾“é‡ï¼‰

### é¢„æœŸæ•ˆæœ
é€šè¿‡ç»„åˆä¼˜åŒ–ï¼Œå¯ä»¥å°†æ€»ä¼ è¾“é‡ä» **7 GB é™è‡³ 1.5 GB**ï¼Œæ€§èƒ½æå‡ **3-5 å€**ã€‚
