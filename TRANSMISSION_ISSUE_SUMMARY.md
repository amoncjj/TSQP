# ä¼ è¾“é‡é—®é¢˜æ€»ç»“

## ğŸ“Š é—®é¢˜ç°çŠ¶

åœ¨ä½¿ç”¨ **LLaMA 3.2-1B** æ¨¡å‹ã€**1024 tokens** prefill çš„æƒ…å†µä¸‹ï¼š

### æ€»ä½“æ•°æ®
- **æ€»ä¼ è¾“é‡**: **13.97 GB** (å‘é€ 6.66 GB + æ¥æ”¶ 7.31 GB)
- **RPC è°ƒç”¨**: 197 æ¬¡
- **å¹³å‡å»¶è¿Ÿ**: 438.77 ms/call
- **æ€»è€—æ—¶**: ~86 ç§’

### æ“ä½œåˆ†å¸ƒ

| æ“ä½œ | æ¬¡æ•° | å‘é€(MB) | æ¥æ”¶(MB) | æ€»é‡(MB) | å æ¯” | å¹³å‡/æ¬¡(MB) |
|-----|------|---------|---------|---------|------|------------|
| **Matmul** | 64 | 4864 | 4352 | **9216** | **66.0%** | **144** |
| BatchLinear | 128 | 1792 | 2944 | 4736 | 33.9% | 37 |
| Embedding | 2 | 0.02 | 16 | 16 | 0.1% | 8 |
| LMHead | 2 | 0.02 | 1 | 1 | 0.0% | 0.5 |

## ğŸ”´ æ ¸å¿ƒé—®é¢˜ï¼šMatmul å  66% ä¼ è¾“é‡

### é—®é¢˜æ ¹æº

**Attention Scores çŸ©é˜µçš„äºŒæ¬¡å¤æ‚åº¦**ï¼š

```python
# Attention è®¡ç®—
Q: [1, 32, 1024, 64]  # 8 MB
K: [1, 32, 1024, 64]  # 8 MB

# é—®é¢˜åœ¨è¿™é‡Œ â†“
Scores = Q @ K^T  # [1, 32, 1024, 1024]  â† 128 MBï¼ï¼ï¼
                  #  â†‘              â†‘
                  #  seq_len Ã— seq_len = äºŒæ¬¡å¤æ‚åº¦

# ç„¶å
Output = Scores @ V  # [1, 32, 1024, 64]  # 8 MB
```

### æ•°å­¦åˆ†æ

**Attention Scores å¤§å°**ï¼š
```
Shape: [batch, num_heads, seq_len, seq_len]
     = [1, 32, 1024, 1024]
     
Size = 1 Ã— 32 Ã— 1024 Ã— 1024 Ã— 4 bytes (float32)
     = 134,217,728 bytes
     = 128 MB

å¤æ‚åº¦: O(seq_lenÂ²)
```

### æ¯å±‚ Attention çš„ä¼ è¾“

```
1. Q @ K^T (Matmul):
   å‘é€: Q (8 MB) + K (8 MB) = 16 MB
   æ¥æ”¶: Scores (128 MB)
   æ€»è®¡: 144 MB

2. Scores @ V (Matmul):
   å‘é€: Scores (128 MB) + V (8 MB) = 136 MB
   æ¥æ”¶: Output (8 MB)
   æ€»è®¡: 144 MB

æ¯å±‚ Attention: 144 + 144 = 288 MB
```

### å…¨æ¨¡å‹ä¼ è¾“é‡

```
æ¨¡å‹ç»“æ„:
- 16 Decoder Layers
- æ¯å±‚åŒ…å«: Attention + MLP

æ¯å±‚ä¼ è¾“é‡:
- Attention: 288 MB (Matmul Ã— 2)
- BatchLinear: ~160 MB (QKV + O + Gate + Up + Down)
- å°è®¡: ~448 MB/layer

å…¨æ¨¡å‹:
- Embedding: 16 MB
- 16 Layers: 448 Ã— 16 = 7168 MB
- LM Head: 1 MB
- æ€»è®¡: ~7.2 GB

å®é™…æµ‹é‡: 13.97 GB (åŒ…å«å¾€è¿”ï¼Œçº¦ 2x)
```

## ğŸ¯ ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤§ï¼Ÿ

### 1. äºŒæ¬¡å¤æ‚åº¦
```
Scores å¤§å° âˆ seq_lenÂ²

seq_len = 1024:
  Scores = 1024Â² Ã— 32 heads Ã— 4 bytes = 128 MB

seq_len = 2048:
  Scores = 2048Â² Ã— 32 heads Ã— 4 bytes = 512 MB  â† 4å€å¢é•¿ï¼
```

### 2. éœ€è¦ä¼ è¾“ä¸­é—´ç»“æœ
```
å½“å‰æ¶æ„:
TEE â”€â”€Q,Kâ”€â”€> GPU â”€â”€Scoresâ”€â”€> TEE â”€â”€Softmaxâ”€â”€> TEE â”€â”€Scoresâ”€â”€> GPU â”€â”€Outputâ”€â”€> TEE
      8MB         128MBâ†é—®é¢˜    128MBâ†’é—®é¢˜         8MB

å¦‚æœ Fused:
TEE â”€â”€Q,K,Vâ”€â”€> GPU â”€â”€[å†…éƒ¨è®¡ç®—]â”€â”€> GPU â”€â”€Outputâ”€â”€> TEE
      24MB                              8MB
```

### 3. Float32 ç²¾åº¦
```
å½“å‰: float32 (4 bytes)
å¯é€‰: bfloat16 (2 bytes) â† å‡åŠï¼
```

## ğŸ’¡ ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼ è¾“é‡ | å‡å°‘ | å®ç°éš¾åº¦ | æ—¶é—´ |
|-----|-------|------|---------|------|
| **å½“å‰ (float32)** | 13.97 GB | - | - | - |
| **1. bfloat16** | 6.98 GB | 50% | â­ ç®€å• | 1å¤© |
| **2. Fused Attention** | 7.52 GB | 46% | â­â­ ä¸­ç­‰ | 1å‘¨ |
| **3. ç»„åˆ (1+2)** | 3.76 GB | 73% | â­â­ ä¸­ç­‰ | 1å‘¨ |
| **4. Flash Attention** | 2.50 GB | 82% | â­â­â­ å›°éš¾ | 1æœˆ |

### æ–¹æ¡ˆ 1: å¯ç”¨ bfloat16 âœ… æ¨èç«‹å³å®æ–½

**åŸç†**: ä½¿ç”¨ 16-bit æµ®ç‚¹æ•°ä»£æ›¿ 32-bit

**ä¿®æ”¹**:
```python
# tee_runner_optimized.py
def init(self) -> Dict:
    meta = {
        "wire_dtype": "bfloat16",  # â† æ”¹è¿™é‡Œ
        "max_chunks": 10,
    }
```

**æ•ˆæœ**:
- ä¼ è¾“é‡: 13.97 GB â†’ **6.98 GB** (å‡å°‘ 50%)
- æ€§èƒ½æå‡: **~2x**
- ç²¾åº¦æŸå¤±: æå°ï¼ˆbfloat16 ä¸“ä¸ºæ·±åº¦å­¦ä¹ è®¾è®¡ï¼‰

**é£é™©**: ä½ï¼ˆä»£ç å·²æ”¯æŒï¼‰

### æ–¹æ¡ˆ 2: Fused Attention â­ æ¨è

**åŸç†**: å°†æ•´ä¸ª Attention è®¡ç®—æ”¾åœ¨ GPU ç«¯ï¼Œä¸ä¼ è¾“ä¸­é—´ Scores

**æ¶æ„å˜åŒ–**:
```python
# å½“å‰
TEE: Reshape, RoPE
GPU: Q@K^T â†’ TEE: Softmax â†’ GPU: Scores@V

# ä¼˜åŒ–å
TEE: Reshape, RoPE
GPU: Q@K^T + Softmax + Scores@V (ä¸€æ¬¡å®Œæˆ)
```

**æ•ˆæœ**:
- Matmul ä¼ è¾“: 9216 MB â†’ **2765 MB** (å‡å°‘ 70%)
- æ€»ä¼ è¾“é‡: 13.97 GB â†’ **7.52 GB** (å‡å°‘ 46%)

**å®ç°**:
```python
# server_optimized.py
@torch.no_grad()
def fused_attention(self, Q, K, V, scaling):
    """Fused Attention - åœ¨ GPU ç«¯å®Œæˆ"""
    scores = torch.matmul(Q, K.transpose(-2, -1)) * scaling
    scores = F.softmax(scores, dim=-1)
    output = torch.matmul(scores, V)
    return output  # åªè¿”å›æœ€ç»ˆç»“æœ
```

### æ–¹æ¡ˆ 3: ç»„åˆä¼˜åŒ– (bfloat16 + Fused Attention) ğŸš€ æœ€ä½³

**æ•ˆæœ**:
- ä¼ è¾“é‡: 13.97 GB â†’ **3.76 GB** (å‡å°‘ 73%)
- æ€§èƒ½æå‡: **3-5x**
- Matmul å æ¯”: 66% â†’ 20%

**å®æ–½æ­¥éª¤**:
1. å¯ç”¨ bfloat16 (1å¤©)
2. å®ç° Fused Attention (3-5å¤©)
3. æµ‹è¯•éªŒè¯ (1-2å¤©)

### æ–¹æ¡ˆ 4: Flash Attention (é•¿æœŸ)

**åŸç†**: ä½¿ç”¨ Flash Attention ç®—æ³•ï¼Œé¿å…æ˜¾å¼è®¡ç®—å®Œæ•´ Scores çŸ©é˜µ

**ä¼˜åŠ¿**:
- å†…å­˜å¤æ‚åº¦: O(NÂ²) â†’ O(N)
- é€Ÿåº¦æ›´å¿«
- ä¼ è¾“é‡æœ€å°

**æŒ‘æˆ˜**:
- éœ€è¦é›†æˆ `flash-attn` åº“
- å¯èƒ½éœ€è¦ä¿®æ”¹æ¨¡å‹ç»“æ„
- è°ƒè¯•å¤æ‚

## ğŸ“ˆ æ€§èƒ½é¢„æµ‹

### å½“å‰æ€§èƒ½
```
Prefill (1024 tokens):
  ä¼ è¾“é‡: 13.97 GB
  æ€»è€—æ—¶: ~86 ç§’
  ååé‡: 162 MB/s
  ç“¶é¢ˆ: Matmul (Scores ä¼ è¾“)
```

### ä¼˜åŒ–å (bfloat16 + Fused Attention)
```
Prefill (1024 tokens):
  ä¼ è¾“é‡: 3.76 GB  (â†“ 73%)
  æ€»è€—æ—¶: ~25 ç§’   (â†“ 71%)
  ååé‡: 450 MB/s (â†‘ 2.8x)
  ç“¶é¢ˆ: BatchLinear
```

### ä¸åŒ seq_len çš„å½±å“

| seq_len | å½“å‰ä¼ è¾“é‡ | ä¼˜åŒ–å | æ”¹è¿› |
|---------|-----------|--------|------|
| 512 | 4.5 GB | 1.5 GB | 3x |
| 1024 | 14.0 GB | 3.8 GB | 3.7x |
| 2048 | 48.0 GB | 10.0 GB | 4.8x |
| 4096 | 180.0 GB | 32.0 GB | 5.6x |

**ç»“è®º**: seq_len è¶Šå¤§ï¼Œä¼˜åŒ–æ•ˆæœè¶Šæ˜æ˜¾ï¼

## ğŸ”§ å®æ–½è®¡åˆ’

### Phase 1: å¿«é€Ÿä¼˜åŒ– (1-2å¤©) âœ…
- [x] åˆ†æé—®é¢˜æ ¹æº
- [ ] å¯ç”¨ bfloat16
- [ ] éªŒè¯ä¼ è¾“é‡å‡åŠ
- [ ] æ€§èƒ½æµ‹è¯•

### Phase 2: æ¶æ„ä¼˜åŒ– (1å‘¨)
- [ ] è®¾è®¡ Fused Attention æ¥å£
- [ ] å®ç°æœåŠ¡ç«¯ fused_attention()
- [ ] ä¿®æ”¹å®¢æˆ·ç«¯è°ƒç”¨é€»è¾‘
- [ ] é›†æˆæµ‹è¯•

### Phase 3: æ€§èƒ½è°ƒä¼˜ (1å‘¨)
- [ ] ä¼˜åŒ– BatchLinear åˆå¹¶
- [ ] åŠ¨æ€é˜ˆå€¼è°ƒæ•´
- [ ] å‹ç¼©ç®—æ³•ä¼˜åŒ–
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•

### Phase 4: é•¿æœŸä¼˜åŒ– (1æœˆ+)
- [ ] è¯„ä¼° Flash Attention
- [ ] é›†æˆ flash-attn åº“
- [ ] æ€§èƒ½å¯¹æ¯”æµ‹è¯•
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

## ğŸ“ ä»£ç ç¤ºä¾‹

### 1. å¯ç”¨ bfloat16 (ç«‹å³å¯è¡Œ)

```python
# tee_gpu/tee_runner_optimized.py
def init(self) -> Dict:
    meta = {
        "wire_dtype": "bfloat16",  # â† ä» float32 æ”¹ä¸º bfloat16
        "max_chunks": 10,
    }
    init_data = self._send_request("Init", meta)
    # ...
```

### 2. Fused Attention (æ¨è)

**æœåŠ¡ç«¯**:
```python
# tee_gpu/server_optimized.py
class GPUComputeService:
    @torch.no_grad()
    def fused_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, 
                       scaling: float) -> torch.Tensor:
        """Fused Attention - åœ¨ GPU ç«¯å®Œæˆæ‰€æœ‰è®¡ç®—"""
        # Q, K, V: [batch, heads, seq_len, head_dim]
        scores = torch.matmul(Q, K.transpose(-2, -1)) * scaling
        scores = F.softmax(scores, dim=-1, dtype=torch.float32).to(Q.dtype)
        output = torch.matmul(scores, V)
        return output

class ZMQServer:
    def handle_fused_attention(self, request: Dict) -> Dict:
        """å¤„ç† Fused Attention è¯·æ±‚"""
        Q = self._receive_tensor(request["Q"])
        K = self._receive_tensor(request["K"])
        V = self._receive_tensor(request["V"])
        scaling = request["scaling"]
        
        output = self.compute.fused_attention(Q, K, V, scaling)
        return {"output": self._send_tensor(output)}
```

**å®¢æˆ·ç«¯**:
```python
# tee_gpu/tee_runner_optimized.py
class GPUClient:
    def fused_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, 
                       scaling: float) -> torch.Tensor:
        """Fused Attention"""
        request = {
            "Q": self._send_tensor(Q),
            "K": self._send_tensor(K),
            "V": self._send_tensor(V),
            "scaling": scaling,
        }
        resp = self._send_request("FusedAttention", request)
        return self._receive_tensor(resp["output"])

class TEELlamaModel:
    def attention(self, layer_idx: int, hidden_states: torch.Tensor, 
                 position_ids: torch.Tensor) -> torch.Tensor:
        # 1. QKV projections (GPU)
        qkv = self.gpu.batch_linear(layer_idx, ["q_proj", "k_proj", "v_proj"], hidden_states)
        Q, K, V = qkv
        
        # 2. Reshape (TEE)
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
        
        # 3. RoPE (TEE)
        cos, sin = self.rotary_emb(V, position_ids)
        Q, K = apply_rotary_pos_emb(Q, K, cos, sin)
        K = repeat_kv(K, self.num_key_value_groups)
        V = repeat_kv(V, self.num_key_value_groups)
        
        # 4. Fused Attention (GPU) â† æ›¿æ¢åŸæ¥çš„ä¸¤æ¬¡ Matmul
        attn_output = self.gpu.fused_attention(Q, K, V, self.scaling)
        
        # 5. Reshape (TEE)
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)
        
        # 6. O projection (GPU)
        attn_output = self.gpu.batch_linear(layer_idx, ["o_proj"], attn_output)[0]
        
        return attn_output
```

## ğŸ¯ æ€»ç»“

### é—®é¢˜æœ¬è´¨
**Attention Scores çš„äºŒæ¬¡å¤æ‚åº¦** å¯¼è‡´ä¼ è¾“é‡å·¨å¤§ï¼š
- Scores: [1, 32, 1024, 1024] = **128 MB**
- æ¯å±‚ä¼ è¾“ 2 æ¬¡ = **288 MB/layer**
- 16 å±‚ = **4.6 GB** (å æ€»ä¼ è¾“é‡çš„ 66%)

### è§£å†³æ–¹æ¡ˆ
1. **ç«‹å³**: å¯ç”¨ bfloat16 â†’ å‡å°‘ 50% ä¼ è¾“é‡
2. **æ¨è**: Fused Attention â†’ å‡å°‘ 70% Matmul ä¼ è¾“
3. **æœ€ä½³**: ç»„åˆä¼˜åŒ– â†’ å‡å°‘ 73% æ€»ä¼ è¾“é‡ï¼Œæ€§èƒ½æå‡ 3-5x

### é¢„æœŸæ•ˆæœ
```
å½“å‰: 13.97 GB, ~86 ç§’
ä¼˜åŒ–: 3.76 GB,  ~25 ç§’  (3.4x åŠ é€Ÿ)
```

### ä¸‹ä¸€æ­¥
1. å¯ç”¨ bfloat16ï¼ˆ1å¤©ï¼‰
2. å®ç° Fused Attentionï¼ˆ1å‘¨ï¼‰
3. æ€§èƒ½æµ‹è¯•å’Œè°ƒä¼˜ï¼ˆ1å‘¨ï¼‰
