"""
ä¼˜åŒ–çš„åŠ å¯†æ–¹æ¡ˆå®ç°
å°†å¤æ‚åº¦ä» O(seq_len^2 Ã— hidden_size) é™ä½åˆ° O(seq_len Ã— hidden_size)
"""
import torch


class OurEncryptionSchemeOptimized:
    """ä¼˜åŒ–çš„åŠ å¯†æ–¹æ¡ˆï¼šåˆ©ç”¨å¯¹è§’çŸ©é˜µæ€§è´¨"""
    
    def __init__(self, seq_len: int, device: torch.device):
        self.seq_len = seq_len
        self.device = device
        
        # ğŸ”‘ å…³é”®ä¼˜åŒ–ï¼šåªå­˜å‚¨å¯¹è§’å…ƒç´ ï¼Œä¸å­˜å‚¨å®Œæ•´çŸ©é˜µ
        self.D_diag = torch.randn(seq_len, device=device) + 2.0  # (seq_len,)
        self.alpha = torch.randn(seq_len, 1, device=device)  # (seq_len, 1)
        self.beta = torch.randn(seq_len, 1, device=device)   # (seq_len, 1)
        
        # é¢„è®¡ç®—é€†çŸ©é˜µçš„å¯¹è§’å…ƒç´ 
        self.D_inv_diag = 1.0 / self.D_diag  # (seq_len,)
        self.D_inv_alpha = self.D_inv_diag.unsqueeze(-1) * self.alpha  # (seq_len, 1)
        self.beta_T_D_inv_alpha = (self.beta.T @ self.D_inv_alpha).item()
        self.scale_factor = 1.0 / (1.0 + self.beta_T_D_inv_alpha)
    
    def encrypt_linear_input(self, X: torch.Tensor) -> torch.Tensor:
        """
        åŠ å¯† Linear å±‚è¾“å…¥: MX = DX + Î±(Î²^T X)
        ä¼˜åŒ–ï¼šåˆ©ç”¨ D æ˜¯å¯¹è§’çŸ©é˜µçš„æ€§è´¨
        
        å¤æ‚åº¦ï¼šO(batch Ã— seq_len Ã— in_features)
        åŸå§‹ï¼šO(batch Ã— seq_len^2 Ã— in_features)
        """
        # X: (batch, seq_len, in_features)
        batch_size, seq_len, in_features = X.shape
        
        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        D_diag = self.D_diag.to(X.dtype)
        alpha = self.alpha.to(X.dtype)
        beta = self.beta.to(X.dtype)
        
        # ğŸš€ ä¼˜åŒ– 1ï¼šå¯¹è§’çŸ©é˜µä¹˜æ³• -> é€å…ƒç´ ä¹˜æ³•
        # DX = D @ Xï¼Œä½† D æ˜¯å¯¹è§’çŸ©é˜µ
        # ç­‰ä»·äºæ¯è¡Œä¹˜ä»¥å¯¹åº”çš„å¯¹è§’å…ƒç´ 
        DX = D_diag.view(1, -1, 1) * X  # å¹¿æ’­ï¼š(1, seq_len, 1) Ã— (batch, seq_len, in_features)
        
        # Î²^T X: (1, seq_len) @ (batch, seq_len, in_features) -> (batch, 1, in_features)
        # ğŸš€ ä¼˜åŒ– 2ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„ einsum
        beta_T_X = torch.einsum('si,bsi->bi', beta.T, X).unsqueeze(1)
        
        # Î±(Î²^T X): (seq_len, 1) Ã— (batch, 1, in_features) -> (batch, seq_len, in_features)
        alpha_beta_T_X = alpha * beta_T_X  # å¹¿æ’­
        
        MX = DX + alpha_beta_T_X
        return MX
    
    def decrypt_linear_output(self, Z: torch.Tensor) -> torch.Tensor:
        """
        è§£å¯† Linear å±‚è¾“å‡º: M^{-1}Z = D^{-1}Z - scale * D^{-1}Î±(Î²^T D^{-1}Z)
        ä¼˜åŒ–ï¼šåˆ©ç”¨ D^{-1} æ˜¯å¯¹è§’çŸ©é˜µçš„æ€§è´¨
        
        å¤æ‚åº¦ï¼šO(batch Ã— seq_len Ã— out_features)
        åŸå§‹ï¼šO(batch Ã— seq_len^2 Ã— out_features)
        """
        # Z: (batch, seq_len, out_features)
        
        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        D_inv_diag = self.D_inv_diag.to(Z.dtype)
        beta = self.beta.to(Z.dtype)
        D_inv_alpha = self.D_inv_alpha.to(Z.dtype)
        
        # ğŸš€ ä¼˜åŒ–ï¼šå¯¹è§’çŸ©é˜µä¹˜æ³•
        # D^{-1}Z
        D_inv_Z = D_inv_diag.view(1, -1, 1) * Z
        
        # Î²^T D^{-1}Z
        beta_T_D_inv_Z = torch.einsum('si,bsi->bi', beta.T, D_inv_Z).unsqueeze(1)
        
        # D^{-1}Î±(Î²^T D^{-1}Z)
        D_inv_alpha_term = D_inv_alpha * beta_T_D_inv_Z
        
        # M^{-1}Z
        M_inv_Z = D_inv_Z - self.scale_factor * D_inv_alpha_term
        return M_inv_Z


class MatmulEncryptionSchemeOptimized:
    """
    ä¼˜åŒ–çš„ Matmul åŠ å¯†æ–¹æ¡ˆ
    1. åˆ©ç”¨å¯¹è§’çŸ©é˜µæ€§è´¨
    2. å»é™¤å•ä½çŸ©é˜µä¹˜æ³•
    3. å‘é‡åŒ–è®¡ç®—ï¼ˆæ— å¾ªç¯ï¼‰
    """
    
    def __init__(self, seq_len: int, head_dim: int, device: torch.device):
        self.seq_len = seq_len
        self.head_dim = head_dim
        self.device = device
        
        # ğŸ”‘ ä¼˜åŒ–ï¼šåªå­˜å‚¨å¯¹è§’å…ƒç´ 
        self.D1_diag = torch.randn(seq_len, device=device) + 2.0
        self.D2_diag = torch.randn(head_dim, device=device) + 2.0
        self.D3_diag = torch.randn(seq_len, device=device) + 2.0
        
        # é¢„è®¡ç®—é€†çŸ©é˜µçš„å¯¹è§’å…ƒç´ 
        self.D1_inv_diag = 1.0 / self.D1_diag
        self.D2_inv_diag = 1.0 / self.D2_diag
        self.D3_inv_diag = 1.0 / self.D3_diag
        
        # ğŸ”‘ ä¼˜åŒ–ï¼šP1, P2, P3 æ˜¯å•ä½çŸ©é˜µï¼Œç›´æ¥è·³è¿‡
        # åŸå§‹ï¼šself.P1 = torch.eye(seq_len)
        # ä¼˜åŒ–ï¼šä¸å­˜å‚¨ï¼ŒåŠ å¯†æ—¶ä¸ä½¿ç”¨
    
    def encrypt_query(self, Q: torch.Tensor) -> torch.Tensor:
        """
        åŠ å¯† Query: Q' = (Dâ‚Pâ‚)Q(Pâ‚‚Dâ‚‚)
        ç”±äº P1=P2=Iï¼Œç®€åŒ–ä¸º: Q' = Dâ‚ Q Dâ‚‚
        
        å¤æ‚åº¦ï¼šO(batch Ã— num_heads Ã— seq_len Ã— head_dim)
        åŸå§‹ï¼šO(batch Ã— num_heads Ã— seq_len^3) + O(... Ã— seq_len^2 Ã— head_dim)
        
        æ€§èƒ½æå‡ï¼š~1000-3000Ã—
        """
        # Q: (batch, num_heads, seq_len, head_dim)
        
        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        D1_diag = self.D1_diag.to(Q.dtype)
        D2_diag = self.D2_diag.to(Q.dtype)
        
        # ğŸš€ å‘é‡åŒ–å¯¹è§’ä¹˜æ³•ï¼ˆæ— å¾ªç¯ï¼ï¼‰
        # D1 ä½œç”¨äº seq_len ç»´åº¦ï¼ˆaxis=-2ï¼‰
        Q_encrypted = Q * D1_diag.view(1, 1, -1, 1)  # å¹¿æ’­
        
        # D2 ä½œç”¨äº head_dim ç»´åº¦ï¼ˆaxis=-1ï¼‰
        Q_encrypted = Q_encrypted * D2_diag.view(1, 1, 1, -1)  # å¹¿æ’­
        
        return Q_encrypted
    
    def encrypt_key_transpose(self, K_T: torch.Tensor) -> torch.Tensor:
        """
        åŠ å¯† Key^T: K'^T = (Dâ‚‚â»Â¹Pâ‚‚â»Â¹)K^T(Pâ‚ƒDâ‚ƒ)
        ç”±äº P2=P3=Iï¼Œç®€åŒ–ä¸º: K'^T = Dâ‚‚â»Â¹ K^T Dâ‚ƒ
        
        å¤æ‚åº¦ï¼šO(batch Ã— num_heads Ã— head_dim Ã— seq_len)
        åŸå§‹ï¼šO(batch Ã— num_heads Ã— (seq_len^3 + head_dim^3))
        """
        # K_T: (batch, num_heads, head_dim, seq_len)
        
        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        D2_inv_diag = self.D2_inv_diag.to(K_T.dtype)
        D3_diag = self.D3_diag.to(K_T.dtype)
        
        # ğŸš€ å‘é‡åŒ–å¯¹è§’ä¹˜æ³•
        # D2_inv ä½œç”¨äº head_dim ç»´åº¦ï¼ˆaxis=-2ï¼‰
        K_T_encrypted = K_T * D2_inv_diag.view(1, 1, -1, 1)
        
        # D3 ä½œç”¨äº seq_len ç»´åº¦ï¼ˆaxis=-1ï¼‰
        K_T_encrypted = K_T_encrypted * D3_diag.view(1, 1, 1, -1)
        
        return K_T_encrypted
    
    def decrypt_matmul_output(self, QK_T_encrypted: torch.Tensor) -> torch.Tensor:
        """
        è§£å¯† Matmul è¾“å‡º: QK^T = Pâ‚â»Â¹Dâ‚â»Â¹Q'K'^TDâ‚ƒâ»Â¹Pâ‚ƒâ»Â¹
        ç”±äº P1=P3=Iï¼Œç®€åŒ–ä¸º: QK^T = Dâ‚â»Â¹ Q'K'^T Dâ‚ƒâ»Â¹
        
        å¤æ‚åº¦ï¼šO(batch Ã— num_heads Ã— seq_len Ã— seq_len)
        åŸå§‹ï¼šO(batch Ã— num_heads Ã— seq_len^3)
        """
        # QK_T_encrypted: (batch, num_heads, seq_len, seq_len)
        
        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        D1_inv_diag = self.D1_inv_diag.to(QK_T_encrypted.dtype)
        D3_inv_diag = self.D3_inv_diag.to(QK_T_encrypted.dtype)
        
        # ğŸš€ å‘é‡åŒ–å¯¹è§’ä¹˜æ³•
        # D1_inv ä½œç”¨äºç¬¬ä¸€ä¸ª seq_len ç»´åº¦ï¼ˆaxis=-2ï¼‰
        QK_T_decrypted = QK_T_encrypted * D1_inv_diag.view(1, 1, -1, 1)
        
        # D3_inv ä½œç”¨äºç¬¬äºŒä¸ª seq_len ç»´åº¦ï¼ˆaxis=-1ï¼‰
        QK_T_decrypted = QK_T_decrypted * D3_inv_diag.view(1, 1, 1, -1)
        
        return QK_T_decrypted


# ============================================================================
# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
# ============================================================================

def benchmark_comparison():
    """å¯¹æ¯”åŸå§‹å®ç°å’Œä¼˜åŒ–å®ç°çš„æ€§èƒ½"""
    import time
    
    # æµ‹è¯•å‚æ•°ï¼ˆæ¨¡æ‹Ÿ LLaMA-2-7Bï¼‰
    batch = 1
    seq_len = 512
    hidden_size = 4096
    num_heads = 32
    head_dim = 128
    device = torch.device("cpu")
    
    print("="*80)
    print("åŠ å¯†æ–¹æ¡ˆæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    print(f"é…ç½®: seq_len={seq_len}, hidden_size={hidden_size}")
    print(f"      num_heads={num_heads}, head_dim={head_dim}")
    print("="*80)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    X = torch.randn(batch, seq_len, hidden_size, device=device)
    Q = torch.randn(batch, num_heads, seq_len, head_dim, device=device)
    
    # ========== Linear åŠ å¯†æµ‹è¯• ==========
    print("\nã€Linear å±‚åŠ å¯†æµ‹è¯•ã€‘")
    
    # åŸå§‹å®ç°ï¼ˆä½¿ç”¨å®Œæ•´çŸ©é˜µï¼‰
    from tee_gpu.tee_runner_ours import OurEncryptionScheme
    enc_original = OurEncryptionScheme(seq_len, device)
    
    t0 = time.perf_counter()
    for _ in range(10):
        _ = enc_original.encrypt_linear_input(X)
    time_original = (time.perf_counter() - t0) / 10
    
    # ä¼˜åŒ–å®ç°
    enc_optimized = OurEncryptionSchemeOptimized(seq_len, device)
    
    t0 = time.perf_counter()
    for _ in range(10):
        _ = enc_optimized.encrypt_linear_input(X)
    time_optimized = (time.perf_counter() - t0) / 10
    
    print(f"åŸå§‹å®ç°: {time_original*1000:.2f} ms")
    print(f"ä¼˜åŒ–å®ç°: {time_optimized*1000:.2f} ms")
    print(f"åŠ é€Ÿæ¯”: {time_original/time_optimized:.1f}Ã—")
    
    # ========== Matmul åŠ å¯†æµ‹è¯• ==========
    print("\nã€Matmul å±‚åŠ å¯†æµ‹è¯•ã€‘")
    
    # åŸå§‹å®ç°
    from tee_gpu.tee_runner_ours import MatmulEncryptionScheme
    matmul_original = MatmulEncryptionScheme(seq_len, head_dim, device)
    
    t0 = time.perf_counter()
    for _ in range(10):
        _ = matmul_original.encrypt_query(Q)
    time_original_matmul = (time.perf_counter() - t0) / 10
    
    # ä¼˜åŒ–å®ç°
    matmul_optimized = MatmulEncryptionSchemeOptimized(seq_len, head_dim, device)
    
    t0 = time.perf_counter()
    for _ in range(10):
        _ = matmul_optimized.encrypt_query(Q)
    time_optimized_matmul = (time.perf_counter() - t0) / 10
    
    print(f"åŸå§‹å®ç°: {time_original_matmul*1000:.2f} ms")
    print(f"ä¼˜åŒ–å®ç°: {time_optimized_matmul*1000:.2f} ms")
    print(f"åŠ é€Ÿæ¯”: {time_original_matmul/time_optimized_matmul:.1f}Ã—")
    
    # ========== æ•´ä½“ä¼°ç®— ==========
    print("\n" + "="*80)
    print("ã€32 å±‚æ€»ä½“æ€§èƒ½ä¼°ç®—ã€‘")
    print("="*80)
    
    # æ¯å±‚çš„åŠ å¯†æ¬¡æ•°
    linear_per_layer = 10  # 5æ¬¡åŠ å¯† + 5æ¬¡è§£å¯†
    matmul_per_layer = 3   # 2æ¬¡åŠ å¯† + 1æ¬¡è§£å¯†
    
    total_original = (linear_per_layer * time_original + 
                     matmul_per_layer * time_original_matmul) * 32
    total_optimized = (linear_per_layer * time_optimized + 
                      matmul_per_layer * time_optimized_matmul) * 32
    
    print(f"åŸå§‹å®ç°æ€»åŠ å¯†æ—¶é—´: {total_original*1000:.2f} ms")
    print(f"ä¼˜åŒ–å®ç°æ€»åŠ å¯†æ—¶é—´: {total_optimized*1000:.2f} ms")
    print(f"èŠ‚çœæ—¶é—´: {(total_original-total_optimized)*1000:.2f} ms")
    print(f"æ•´ä½“åŠ é€Ÿæ¯”: {total_original/total_optimized:.1f}Ã—")
    print("="*80)


if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½å¯¹æ¯”
    benchmark_comparison()

