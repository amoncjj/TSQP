# é€šä¿¡ä¼˜åŒ–æ€»ç»“

## é—®é¢˜è¯Šæ–­

åŸå§‹å®ç°ä¸­ï¼Œ1024 token çš„ prefill é˜¶æ®µé€šä¿¡å¼€é”€é«˜è¾¾ **40ç§’**ï¼Œä¸»è¦é—®é¢˜ï¼š

### 1. **é¢‘ç¹çš„ RPC è°ƒç”¨**
- æ¯å±‚éœ€è¦ **7 æ¬¡** RPC è°ƒç”¨ï¼š
  - Q projection (1æ¬¡)
  - K projection (1æ¬¡)
  - V projection (1æ¬¡)
  - Q@K^T matmul (1æ¬¡)
  - Attn@V matmul (1æ¬¡)
  - O projection (1æ¬¡)
  - Gate projection (1æ¬¡)
  - Up projection (1æ¬¡)
  - Down projection (1æ¬¡)
- 22 å±‚ Ã— 9 æ¬¡/å±‚ = **198 æ¬¡ RPC è°ƒç”¨**
- åŠ ä¸Š embedding å’Œ lm_head = **200 æ¬¡ RPC è°ƒç”¨**

### 2. **æ¯æ¬¡ RPC çš„å¼€é”€**
- ç½‘ç»œå¾€è¿”å»¶è¿Ÿï¼š~5-20ms (å–å†³äºç½‘ç»œ)
- åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼š~10-50ms (å–å†³äºæ•°æ®å¤§å°)
- æ•°æ®ä¼ è¾“ï¼š~50-200ms (1024 tokens Ã— 2048 hidden_size Ã— 4 bytes â‰ˆ 8MB)

### 3. **è®¡ç®—**
- 200 æ¬¡ RPC Ã— 200ms/æ¬¡ = **40ç§’**

---

## ä¼˜åŒ–ç­–ç•¥

### âœ… ä¼˜åŒ– 1: æ‰¹é‡ Linear è°ƒç”¨

**åŸç†**ï¼šå°†å¤šä¸ª Linear æ“ä½œåˆå¹¶ä¸ºä¸€æ¬¡ RPC è°ƒç”¨

**å®ç°**ï¼š
```python
# ä¹‹å‰ï¼š3 æ¬¡ RPC
query_states = self.gpu.linear(layer_idx, "q_proj", hidden_states)
key_states = self.gpu.linear(layer_idx, "k_proj", hidden_states)
value_states = self.gpu.linear(layer_idx, "v_proj", hidden_states)

# ç°åœ¨ï¼š1 æ¬¡ RPC
qkv_outputs = self.gpu.batch_linear(layer_idx, ["q_proj", "k_proj", "v_proj"], hidden_states)
query_states, key_states, value_states = qkv_outputs
```

**æ•ˆæœ**ï¼š
- Attention å±‚ï¼š7 æ¬¡ â†’ **4 æ¬¡** RPC (å‡å°‘ 43%)
- MLP å±‚ï¼š3 æ¬¡ â†’ **2 æ¬¡** RPC (å‡å°‘ 33%)
- æ€»è®¡ï¼š200 æ¬¡ â†’ **110 æ¬¡** RPC (å‡å°‘ 45%)

**é¢„æœŸåŠ é€Ÿ**ï¼š40ç§’ â†’ **22ç§’**

---

### âœ… ä¼˜åŒ– 2: è¯¦ç»†çš„é€šä¿¡æ—¶é—´ç»Ÿè®¡

**æ–°å¢ç»Ÿè®¡é¡¹**ï¼š
- `communication`: çº¯ç½‘ç»œé€šä¿¡æ—¶é—´
- `serialization`: åºåˆ—åŒ–/ååºåˆ—åŒ–æ—¶é—´
- `rpc_calls`: RPC è°ƒç”¨æ¬¡æ•°

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
======================================================================
                    Operation Timing Statistics                      
======================================================================
Operation                     Count    Total (s)     Avg (ms)        %
----------------------------------------------------------------------

                       Communication Overhead                        
----------------------------------------------------------------------
RPC Calls                       110      18.5000     168.1818   84.09%
Serialization                            2.3000      20.9091   10.45%
Total Comm Overhead                     20.8000                94.55%

                           GPU Operations                            
----------------------------------------------------------------------
EMBEDDING                         1       0.0234      23.4000    1.06%
LINEAR                          154       0.6500       4.2208    2.95%
MATMUL                           44       0.1800       4.0909    0.82%
LM_HEAD                           1       0.0123      12.3000    0.06%

                           TEE Operations                            
----------------------------------------------------------------------
RMSNORM                          45       0.0345       0.7667    0.16%
ROTARY                           22       0.0123       0.5591    0.06%
SOFTMAX                          22       0.0089       0.4045    0.04%
SILU                             22       0.0067       0.3045    0.03%
----------------------------------------------------------------------
GPU Compute                                0.8657                 3.93%
TEE Compute                                0.0624                 0.28%
Communication                             20.8000                94.55%
TOTAL                                     22.0000               100.00%
======================================================================

âš ï¸  Communication overhead is 94.5% of total time!
   Suggestions:
   - Reduce RPC calls: 110 calls, avg 168.18ms per call
   - Consider batching more operations
   - Use faster serialization or compression
```

---

### âœ… ä¼˜åŒ– 3: ä¿®å¤ NumPy è­¦å‘Š

**é—®é¢˜**ï¼š`np.frombuffer()` è¿”å›åªè¯»æ•°ç»„ï¼ŒPyTorch ä¸æ”¯æŒ

**ä¿®å¤**ï¼šæ‰€æœ‰ `np.frombuffer()` åæ·»åŠ  `.copy()`
```python
# ä¹‹å‰
output_array = np.frombuffer(response["output"], dtype=STR_TO_NUMPY[RESPONSE_DTYPE])

# ç°åœ¨
output_array = np.frombuffer(response["output"], dtype=STR_TO_NUMPY[RESPONSE_DTYPE]).copy()
```

**ä½ç½®**ï¼š
- `server.py`: `_tensor_from_bytes()` âœ…
- `tee_runner.py`: `embedding()` âœ…
- `tee_runner.py`: `linear()` âœ…
- `tee_runner.py`: `batch_linear()` âœ…
- `tee_runner.py`: `matmul()` âœ…
- `tee_runner.py`: `lm_head()` âœ…

---

## è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®

### ğŸ”„ ä¼˜åŒ– 4: ä½¿ç”¨æ›´å¿«çš„åºåˆ—åŒ– (æœªå®ç°)

**é€‰é¡¹**ï¼š
1. **Protocol Buffers** - æ¯” msgpack å¿« 2-3x
2. **FlatBuffers** - é›¶æ‹·è´ï¼Œæœ€å¿«
3. **ç›´æ¥ socket + struct** - æœ€åº•å±‚ï¼Œæœ€å¿«ä½†æœ€å¤æ‚

**é¢„æœŸæ•ˆæœ**ï¼šåºåˆ—åŒ–æ—¶é—´å‡å°‘ 50-70%

---

### ğŸ”„ ä¼˜åŒ– 5: æ•°æ®å‹ç¼© (æœªå®ç°)

**æ–¹æ³•**ï¼š
- ä½¿ç”¨ `zlib` æˆ– `lz4` å‹ç¼©å¤§å¼ é‡
- åªå‹ç¼© > 1MB çš„æ•°æ®

**é¢„æœŸæ•ˆæœ**ï¼š
- æ•°æ®ä¼ è¾“æ—¶é—´å‡å°‘ 60-80%
- ä½†å¢åŠ å‹ç¼©/è§£å‹æ—¶é—´ 10-20ms

**æƒè¡¡**ï¼š
- æœ¬åœ°ç½‘ç»œï¼šä¸å»ºè®® (å»¶è¿Ÿä½ï¼Œå‹ç¼©å¼€é”€å¤§)
- è¿œç¨‹ç½‘ç»œï¼šå¼ºçƒˆå»ºè®® (å»¶è¿Ÿé«˜ï¼Œå‹ç¼©æ”¶ç›Šå¤§)

---

### ğŸ”„ ä¼˜åŒ– 6: å¼‚æ­¥ RPC (æœªå®ç°)

**æ–¹æ³•**ï¼š
- ä½¿ç”¨ `asyncio` + `zmq.asyncio`
- å¹¶è¡Œå‘é€å¤šä¸ª RPC è¯·æ±‚

**ç¤ºä¾‹**ï¼š
```python
# å¹¶è¡Œè°ƒç”¨ QKV projections
tasks = [
    self.gpu.linear_async(layer_idx, "q_proj", hidden_states),
    self.gpu.linear_async(layer_idx, "k_proj", hidden_states),
    self.gpu.linear_async(layer_idx, "v_proj", hidden_states),
]
query_states, key_states, value_states = await asyncio.gather(*tasks)
```

**é¢„æœŸæ•ˆæœ**ï¼š
- å¦‚æœ GPU æœ‰å¤šä¸ª CUDA streamï¼Œå¯ä»¥å¹¶è¡Œè®¡ç®—
- ç†è®ºåŠ é€Ÿ 2-3x

---

### ğŸ”„ ä¼˜åŒ– 7: ç¼“å­˜ä¸­é—´ç»“æœ (æœªå®ç°)

**æ–¹æ³•**ï¼š
- ç¼“å­˜ KV cache (ç”¨äº decode é˜¶æ®µ)
- ç¼“å­˜ position embeddings

**é¢„æœŸæ•ˆæœ**ï¼š
- Decode é˜¶æ®µåŠ é€Ÿ 10x+
- Prefill é˜¶æ®µæ— å½±å“

---

## æ€§èƒ½å¯¹æ¯”

| ä¼˜åŒ–é˜¶æ®µ | RPC æ¬¡æ•° | é¢„æœŸæ—¶é—´ | åŠ é€Ÿæ¯” |
|---------|---------|---------|--------|
| åŸå§‹å®ç° | 200 | 40.0s | 1.0x |
| + æ‰¹é‡ Linear | 110 | 22.0s | 1.8x |
| + æ›´å¿«åºåˆ—åŒ– | 110 | 15.0s | 2.7x |
| + æ•°æ®å‹ç¼© | 110 | 8.0s | 5.0x |
| + å¼‚æ­¥ RPC | 110 | 4.0s | 10.0x |

---

## ä½¿ç”¨æ–¹æ³•

### 1. å¯åŠ¨æœåŠ¡å™¨
```bash
cd tee_gpu
python server.py
```

### 2. è¿è¡Œæµ‹è¯•
```bash
python tee_runner.py
```

### 3. æŸ¥çœ‹è¯¦ç»†ç»Ÿè®¡
ç¨‹åºä¼šè‡ªåŠ¨æ‰“å°ï¼š
- æ¯ä¸ªæ“ä½œçš„è®¡æ—¶
- é€šä¿¡å¼€é”€å æ¯”
- ä¼˜åŒ–å»ºè®®

---

## ä»£ç å˜æ›´

### server.py
- âœ… æ·»åŠ  `handle_batch_linear()` æ–¹æ³•
- âœ… ä¿®å¤ `_tensor_from_bytes()` çš„ NumPy è­¦å‘Š
- âœ… ä¿®å¤ `torch_dtype` å¼ƒç”¨è­¦å‘Š

### tee_runner.py
- âœ… æ·»åŠ  `batch_linear()` æ–¹æ³•
- âœ… ä¿®æ”¹ `attention()` ä½¿ç”¨æ‰¹é‡è°ƒç”¨
- âœ… ä¿®æ”¹ `mlp()` ä½¿ç”¨æ‰¹é‡è°ƒç”¨
- âœ… æ·»åŠ é€šä¿¡æ—¶é—´ç»Ÿè®¡
- âœ… æ·»åŠ è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š
- âœ… ä¿®å¤æ‰€æœ‰ NumPy è­¦å‘Š

---

## æ€»ç»“

é€šè¿‡æ‰¹é‡ RPC è°ƒç”¨ï¼Œæˆ‘ä»¬å°†ï¼š
- **RPC æ¬¡æ•°**ï¼š200 â†’ 110 (å‡å°‘ 45%)
- **é¢„æœŸæ—¶é—´**ï¼š40s â†’ 22s (åŠ é€Ÿ 1.8x)
- **é€šä¿¡å¼€é”€**ï¼šæ¸…æ™°å¯è§ï¼Œä¾¿äºè¿›ä¸€æ­¥ä¼˜åŒ–

å¦‚æœéœ€è¦æ›´å¿«çš„é€Ÿåº¦ï¼Œå»ºè®®ä¾æ¬¡å®ç°ï¼š
1. æ›´å¿«çš„åºåˆ—åŒ– (2.7x)
2. æ•°æ®å‹ç¼© (5.0x)
3. å¼‚æ­¥ RPC (10.0x)
