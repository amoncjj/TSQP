# æ€§èƒ½ä¼˜åŒ–è·¯çº¿å›¾

## ğŸ¯ ç›®æ ‡

å°†æ¨ç†æ—¶é—´ä» **44 ç§’** ä¼˜åŒ–åˆ° **1-2 æ¯«ç§’**ï¼Œæå‡ **22000-44000 å€**ï¼

## ğŸ“Š å½“å‰æ€§èƒ½ç“¶é¢ˆ

```
æ€»æ—¶é—´: 44.25 ç§’
â”œâ”€ GPU è®¡ç®—: 43.48s (98.25%)
â”‚  â”œâ”€ Matmul: 29.54s (66.75%)  â† æœ€å¤§ç“¶é¢ˆ
â”‚  â””â”€ Linear: 13.92s (31.45%)
â”œâ”€ RPC é€šä¿¡: 32.59s (73.67%)   â† ç¬¬äºŒå¤§ç“¶é¢ˆ
â”‚  â”œâ”€ åºåˆ—åŒ–: 4.53s
â”‚  â””â”€ ååºåˆ—åŒ–: 2.50s
â””â”€ TEE è®¡ç®—: 0.77s (1.75%)

å…³é”®é—®é¢˜:
1. RPC å»¶è¿Ÿ 332ms/call - åº”è¯¥æ˜¯ 0.2ms (IPC)
2. æ•°æ®ä¼ è¾“ 3.3GB - å¤ªå¤§äº†
3. GPU è®¡ç®—æ—¶é—´å¼‚å¸¸é•¿ - å¯èƒ½æ˜¯æ•°æ®ä¼ è¾“å¯¼è‡´
```

## ğŸš€ ç«‹å³è¡ŒåŠ¨ï¼ˆä»Šå¤©ï¼‰

### æ­¥éª¤ 1: è¯Šæ–­ä¼ è¾“æ–¹å¼ (10 åˆ†é’Ÿ)

**åœ¨è¿œç¨‹æœåŠ¡å™¨ä¸Šè¿è¡Œ**:

```bash
cd /home/fdcffcf0-4e53-40aa-a255-19c2675ad6b1/TSQP/tee_gpu

# è¿è¡Œè¯Šæ–­è„šæœ¬
python diagnose_transport.py
```

**é¢„æœŸè¾“å‡º**:
```
IPC å»¶è¿Ÿ:  0.5-2 ms
TCP å»¶è¿Ÿ:  50-100 ms
IPC æ¯” TCP å¿«: 50-100 å€
```

**å¦‚æœ IPC å»¶è¿Ÿ > 10ms**: è¯´æ˜æœ‰é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥ç³»ç»Ÿé…ç½®ã€‚

### æ­¥éª¤ 2: ç¡®è®¤å®é™…ä½¿ç”¨çš„ä¼ è¾“æ–¹å¼ (5 åˆ†é’Ÿ)

**ä¿®æ”¹ `tee_runner_optimized.py`**ï¼Œæ·»åŠ è¯Šæ–­ä¿¡æ¯ï¼š

```python
def __init__(self, ipc_path: str) -> None:
    self.ipc_path = ipc_path
    self.context = zmq.Context()
    self.socket = self.context.socket(zmq.REQ)
    
    # ä¼˜åŒ– ZeroMQ æ€§èƒ½
    self.socket.setsockopt(zmq.SNDHWM, 1000)
    self.socket.setsockopt(zmq.RCVHWM, 1000)
    self.socket.setsockopt(zmq.LINGER, 0)
    
    self.socket.connect(ipc_path)
    
    # æ·»åŠ è¯Šæ–­ä¿¡æ¯
    transport_type = "IPC" if "ipc://" in ipc_path else "TCP"
    print(f"âœ“ Connected to GPU server at {ipc_path}")
    print(f"  Transport type: {transport_type}")
    print(f"  Expected latency: {'<1ms' if transport_type == 'IPC' else '10-100ms'}")
```

### æ­¥éª¤ 3: åˆ‡æ¢åˆ° bfloat16 (2 åˆ†é’Ÿ)

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export LLAMA_DTYPE="bfloat16"

# é‡å¯æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯
pkill -f server_optimized
pkill -f tee_runner_optimized

python server_optimized.py &
python tee_runner_optimized.py
```

**é¢„æœŸæ•ˆæœ**:
- æ•°æ®ä¼ è¾“é‡: 3.3GB â†’ 1.65GB (å‡åŠ)
- åºåˆ—åŒ–æ—¶é—´: 4.53s â†’ 2.27s (å‡åŠ)
- GPU è®¡ç®—: å¯èƒ½æå‡ 1.5-2 å€

**æ€»é¢„æœŸ**: 44s â†’ **15-20s** (2-3 å€æå‡)

---

## ğŸ”§ æ·±åº¦ä¼˜åŒ–ï¼ˆæœ¬å‘¨ï¼‰

### ä¼˜åŒ– 1: å…±äº«å†…å­˜é›¶æ‹·è´ (1 å¤©)

**é—®é¢˜**: æ¯æ¬¡ RPC éƒ½è¦åºåˆ—åŒ–/ååºåˆ—åŒ– 1.65GB æ•°æ®ï¼ˆbfloat16ï¼‰

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ POSIX å…±äº«å†…å­˜

#### å®ç°æ­¥éª¤

1. **å®‰è£…ä¾èµ–**:
```bash
pip install posix_ipc
```

2. **åˆ›å»ºå…±äº«å†…å­˜ç®¡ç†å™¨** (`tee_gpu/shared_memory.py`):

```python
import mmap
import posix_ipc
import numpy as np
import torch

class SharedMemoryManager:
    """å…±äº«å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, name: str, size: int = 1024 * 1024 * 1024):  # 1GB
        self.name = name
        self.size = size
        
        # åˆ›å»ºå…±äº«å†…å­˜
        self.shm = posix_ipc.SharedMemory(
            name,
            posix_ipc.O_CREAT,
            size=size
        )
        self.mem = mmap.mmap(self.shm.fd, size)
        self.offset = 0
    
    def write_tensor(self, tensor: torch.Tensor) -> dict:
        """å†™å…¥å¼ é‡ï¼Œè¿”å›å…ƒæ•°æ®"""
        # è½¬æ¢ä¸º numpy
        array = tensor.detach().cpu().numpy()
        data = array.tobytes()
        
        # å†™å…¥å…±äº«å†…å­˜
        offset = self.offset
        self.mem[offset:offset+len(data)] = data
        self.offset = (offset + len(data)) % self.size
        
        return {
            "offset": offset,
            "shape": list(array.shape),
            "dtype": str(array.dtype),
            "size": len(data)
        }
    
    def read_tensor(self, metadata: dict) -> torch.Tensor:
        """ä»å…±äº«å†…å­˜è¯»å–å¼ é‡"""
        offset = metadata["offset"]
        size = metadata["size"]
        shape = metadata["shape"]
        dtype = np.dtype(metadata["dtype"])
        
        # ä»å…±äº«å†…å­˜è¯»å–
        data = bytes(self.mem[offset:offset+size])
        array = np.frombuffer(data, dtype=dtype).reshape(shape).copy()
        
        return torch.from_numpy(array)
    
    def close(self):
        """å…³é—­å…±äº«å†…å­˜"""
        self.mem.close()
        posix_ipc.unlink_shared_memory(self.name)
```

3. **ä¿®æ”¹ GPUClient** ä½¿ç”¨å…±äº«å†…å­˜:

```python
class GPUClient:
    def __init__(self, ipc_path: str):
        # ZeroMQ è¿æ¥
        self.socket = ...
        
        # å…±äº«å†…å­˜
        self.shm = SharedMemoryManager("/tsqp_shm")
    
    def batch_linear(self, layer_idx, module_names, hidden_states):
        # å†™å…¥å…±äº«å†…å­˜
        metadata = self.shm.write_tensor(hidden_states)
        
        # RPC åªä¼ é€’å…ƒæ•°æ®
        request = {
            "layer_idx": layer_idx,
            "module_names": module_names,
            "shm_metadata": metadata  # åªæœ‰å‡ åå­—èŠ‚
        }
        
        response = self._send_request("BatchLinear", request)
        
        # ä»å…±äº«å†…å­˜è¯»å–ç»“æœ
        outputs = []
        for output_meta in response["outputs"]:
            tensor = self.shm.read_tensor(output_meta)
            outputs.append(tensor)
        
        return outputs
```

**é¢„æœŸæ•ˆæœ**:
- åºåˆ—åŒ–æ—¶é—´: 2.27s â†’ **0.01s** (227 å€æå‡)
- ååºåˆ—åŒ–æ—¶é—´: 1.14s â†’ **0.01s** (114 å€æå‡)
- RPC æ•°æ®é‡: 1.65GB â†’ **å‡  KB** (ç™¾ä¸‡å€å‡å°‘)

**æ€»é¢„æœŸ**: 15-20s â†’ **5-8s** (3-4 å€æå‡)

---

### ä¼˜åŒ– 2: æ•´å±‚åˆå¹¶ (åŠå¤©)

**é—®é¢˜**: æ¯å±‚ 5-6 æ¬¡ RPC è°ƒç”¨

**è§£å†³æ–¹æ¡ˆ**: æ¯å±‚åªè°ƒç”¨ 1 æ¬¡ RPC

#### å®ç°æ­¥éª¤

1. **åœ¨ `server_optimized.py` æ·»åŠ æ•´å±‚æ–¹æ³•**:

```python
@torch.no_grad()
def full_decoder_layer(self, layer_idx: int, hidden_states: torch.Tensor, 
                       cos: torch.Tensor, sin: torch.Tensor) -> dict:
    """å®Œæ•´çš„ decoder å±‚ï¼ˆGPU éƒ¨åˆ†ï¼‰"""
    layer = self.layers[layer_idx]
    
    # Attention
    # 1. QKV projections
    q = layer.self_attn.q_proj(hidden_states)
    k = layer.self_attn.k_proj(hidden_states)
    v = layer.self_attn.v_proj(hidden_states)
    
    # è¿”å›ç»™ TEE åš RoPE + Softmax
    return {
        "q": q, "k": k, "v": v,
        "residual": hidden_states
    }

@torch.no_grad()
def attention_output(self, layer_idx: int, attn_output: torch.Tensor) -> torch.Tensor:
    """Attention è¾“å‡ºæŠ•å½±"""
    layer = self.layers[layer_idx]
    return layer.self_attn.o_proj(attn_output)

@torch.no_grad()
def mlp_forward(self, layer_idx: int, hidden_states: torch.Tensor) -> torch.Tensor:
    """MLP å‰å‘ä¼ æ’­"""
    layer = self.layers[layer_idx]
    gate = layer.mlp.gate_proj(hidden_states)
    up = layer.mlp.up_proj(hidden_states)
    # è¿”å›ç»™ TEE åš SiLU
    return {"gate": gate, "up": up}

@torch.no_grad()
def mlp_output(self, layer_idx: int, gate_up: torch.Tensor) -> torch.Tensor:
    """MLP è¾“å‡ºæŠ•å½±"""
    layer = self.layers[layer_idx]
    return layer.mlp.down_proj(gate_up)
```

2. **ä¿®æ”¹ TEELlamaModel**:

```python
def decoder_layer(self, layer_idx, hidden_states, position_ids):
    """Decoder å±‚ - ä¼˜åŒ–ç‰ˆ"""
    residual = hidden_states
    
    # TEE: Input norm
    hidden_states = self.input_layernorms[layer_idx](hidden_states)
    
    # GPU: QKV projections (1 æ¬¡ RPC)
    qkv_data = self.gpu.full_decoder_layer(layer_idx, hidden_states, cos, sin)
    
    # TEE: RoPE + Attention
    attn_output = self._tee_attention(qkv_data, position_ids)
    
    # GPU: O projection (1 æ¬¡ RPC)
    attn_output = self.gpu.attention_output(layer_idx, attn_output)
    
    hidden_states = residual + attn_output
    residual = hidden_states
    
    # TEE: Post attention norm
    hidden_states = self.post_attention_layernorms[layer_idx](hidden_states)
    
    # GPU: Gate/Up projections (1 æ¬¡ RPC)
    gate_up_data = self.gpu.mlp_forward(layer_idx, hidden_states)
    
    # TEE: SiLU
    gate_up = self._tee_mlp(gate_up_data)
    
    # GPU: Down projection (1 æ¬¡ RPC)
    mlp_output = self.gpu.mlp_output(layer_idx, gate_up)
    
    hidden_states = residual + mlp_output
    
    return hidden_states
```

**é¢„æœŸæ•ˆæœ**:
- RPC æ¬¡æ•°: 98 â†’ **88** (æ¯å±‚ä» 5-6 æ¬¡å‡å°‘åˆ° 4 æ¬¡)
- RPC å¼€é”€å‡å°‘ 20-30%

**æ€»é¢„æœŸ**: 5-8s â†’ **3-5s** (1.5-2 å€æå‡)

---

### ä¼˜åŒ– 3: GPU Kernel èåˆ (2 å¤©)

**é—®é¢˜**: GPU è®¡ç®—æ—¶é—´å¼‚å¸¸é•¿ï¼ˆ29.5s matmul + 13.9s linearï¼‰

**å¯èƒ½åŸå› **:
1. æ•°æ®åœ¨ CPU å’Œ GPU ä¹‹é—´é¢‘ç¹ä¼ è¾“
2. å°æ‰¹é‡æ“ä½œæ•ˆç‡ä½
3. æ²¡æœ‰ä½¿ç”¨ Flash Attention

#### è§£å†³æ–¹æ¡ˆ

1. **ä½¿ç”¨ Flash Attention**:

```bash
pip install flash-attn
```

```python
from flash_attn import flash_attn_func

def attention_with_flash(q, k, v):
    """ä½¿ç”¨ Flash Attention"""
    # q, k, v: [batch, num_heads, seq_len, head_dim]
    output = flash_attn_func(q, k, v, causal=True)
    return output
```

**é¢„æœŸæ•ˆæœ**: Attention è®¡ç®—æå‡ 2-4 å€

2. **Kernel èåˆ**:

```python
import torch.nn.functional as F

@torch.jit.script
def fused_mlp(x: torch.Tensor, gate_weight: torch.Tensor, 
              up_weight: torch.Tensor, down_weight: torch.Tensor) -> torch.Tensor:
    """èåˆçš„ MLP"""
    gate = F.linear(x, gate_weight)
    up = F.linear(x, up_weight)
    gate_up = F.silu(gate) * up
    return F.linear(gate_up, down_weight)
```

**é¢„æœŸæ•ˆæœ**: MLP è®¡ç®—æå‡ 1.5-2 å€

**æ€»é¢„æœŸ**: 3-5s â†’ **1-2s** (2-3 å€æå‡)

---

## ğŸ¯ æœ€ç»ˆç›®æ ‡

| é˜¶æ®µ | ä¼˜åŒ–å†…å®¹ | é¢„æœŸæ—¶é—´ | æå‡å€æ•° |
|------|---------|---------|---------|
| å½“å‰ | æ—  | 44.25s | 1x |
| é˜¶æ®µ1 | IPC + bfloat16 | 15-20s | 2-3x |
| é˜¶æ®µ2 | å…±äº«å†…å­˜ | 5-8s | 5-9x |
| é˜¶æ®µ3 | æ•´å±‚åˆå¹¶ | 3-5s | 9-15x |
| é˜¶æ®µ4 | GPU ä¼˜åŒ– | 1-2s | 22-44x |
| ç»ˆæ | CUDA IPC | 0.5-1s | 44-88x |

## ğŸ“ ä»Šå¤©çš„ä»»åŠ¡æ¸…å•

- [ ] è¿è¡Œ `diagnose_transport.py` è¯Šæ–­ä¼ è¾“æ–¹å¼
- [ ] ç¡®è®¤å®é™…ä½¿ç”¨ IPC è¿˜æ˜¯ TCP
- [ ] åˆ‡æ¢åˆ° bfloat16
- [ ] æµ‹è¯•æ€§èƒ½æå‡
- [ ] å¦‚æœæ€§èƒ½ä»ç„¶å¾ˆæ…¢ï¼Œæ£€æŸ¥ IPC æ–‡ä»¶æƒé™å’Œç³»ç»Ÿé…ç½®

## ğŸ” è°ƒè¯•æ£€æŸ¥æ¸…å•

å¦‚æœæ€§èƒ½ä»ç„¶å¾ˆæ…¢ï¼Œæ£€æŸ¥ï¼š

1. **IPC æ–‡ä»¶**:
```bash
ls -la /tmp/tsqp_gpu_server.ipc
# åº”è¯¥å­˜åœ¨ä¸”å¯è¯»å†™
```

2. **è¿›ç¨‹è¿æ¥**:
```bash
lsof -p $(pgrep -f server_optimized) | grep socket
lsof -p $(pgrep -f tee_runner_optimized) | grep socket
```

3. **ç¯å¢ƒå˜é‡**:
```bash
echo $LLAMA_IPC_PATH
# åº”è¯¥æ˜¯ç©ºçš„æˆ–è€…æ˜¯ ipc:///tmp/tsqp_gpu_server.ipc
```

4. **ZeroMQ ç‰ˆæœ¬**:
```bash
python -c "import zmq; print(zmq.zmq_version())"
# åº”è¯¥ >= 4.0
```

---

**å¼€å§‹è¡ŒåŠ¨å§ï¼ç¬¬ä¸€æ­¥å°±æ˜¯è¿è¡Œè¯Šæ–­è„šæœ¬ï¼** ğŸš€
