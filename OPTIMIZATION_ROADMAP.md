# æ€§èƒ½ä¼˜åŒ–è·¯çº¿å›¾ V2 (åŸºäºå®é™…æµ‹é‡)

## ğŸ“Š å½“å‰çŠ¶æ€

**å®æµ‹æ€§èƒ½**: 332ms/token
**ç›®æ ‡æ€§èƒ½**: 10ms/token
**æå‡å€æ•°**: 33x

### æ€§èƒ½åˆ†è§£

```
æ€»å»¶è¿Ÿ: 332ms (100%)
â”œâ”€ åºåˆ—åŒ–/ååºåˆ—åŒ–: 136ms (41%)  â† æœ€å¤§ç“¶é¢ˆ
â”œâ”€ GPUè®¡ç®—: 100ms (30%)
â”œâ”€ RPCå¼€é”€: 50ms (15%)
â””â”€ TEEè®¡ç®—: 46ms (14%)

RPCç»Ÿè®¡:
- æ€»è°ƒç”¨æ¬¡æ•°: 98æ¬¡
- å¹³å‡å»¶è¿Ÿ: 3.4ms/æ¬¡
- æ€»æ•°æ®é‡: 7.5GB
```

### å…³é”®å‘ç°

1. **è¯Šæ–­æµ‹è¯•è¯¯å¯¼**: å•æ¬¡10MBä¼ è¾“21ms,ä½†å®é™…98æ¬¡è°ƒç”¨332ms
2. **çœŸæ­£ç“¶é¢ˆ**: åºåˆ—åŒ– > GPUè®¡ç®— > RPCæ¬¡æ•° > ä¼ è¾“åè®®
3. **IPCä¼˜åŠ¿æœ‰é™**: å¯¹å¤§æ•°æ®(10MB+),IPC vs TCPå·®å¼‚<2%

è¯¦è§: [æ€§èƒ½å·®è·åˆ†æ](PERFORMANCE_GAP_ANALYSIS.md)

---

## ğŸ¯ ä¼˜åŒ–é˜¶æ®µ

### é˜¶æ®µ 1: å…±äº«å†…å­˜ (1-2å¤©)

**ç›®æ ‡**: 332ms â†’ 206ms (1.6xæå‡)

#### æ ¸å¿ƒæ€è·¯

æ¶ˆé™¤msgpackåºåˆ—åŒ–å¼€é”€,ä½¿ç”¨å…±äº«å†…å­˜é›¶æ‹·è´ä¼ è¾“

#### å®ç°æ–¹æ¡ˆ

```python
# 1. åˆ›å»ºå…±äº«å†…å­˜
import posix_ipc
import mmap

shm = posix_ipc.SharedMemory("/tsqp_shm", size=100*1024*1024)
mem = mmap.mmap(shm.fd, shm.size)

# 2. å†™å…¥æ•°æ®(é›¶æ‹·è´)
offset = 0
data = array.tobytes()
mem[offset:offset+len(data)] = data

# 3. åªä¼ è¾“å…ƒæ•°æ®
metadata = {
    "shm_offset": offset,
    "shape": [1, 1024, 2048],
    "dtype": "float32"
}
message = msgpack.packb({"method": "BatchLinear", "metadata": metadata})
```

#### é¢„æœŸæ•ˆæœ

- åºåˆ—åŒ–: 136ms â†’ 10ms (13.6x)
- å†…å­˜æ‹·è´: å¤§å¹…å‡å°‘
- **æ€»å»¶è¿Ÿ**: 332ms â†’ 206ms

#### å®ç°æ–‡ä»¶

- `tee_gpu/shared_memory.py` - å…±äº«å†…å­˜ç®¡ç†å™¨
- ä¿®æ”¹ `GPUClient._send_request()` - ä½¿ç”¨å…±äº«å†…å­˜
- ä¿®æ”¹ `GPUServer.handle_*()` - ä»å…±äº«å†…å­˜è¯»å–

---

### é˜¶æ®µ 2: ç®—å­èåˆ (2-3å¤©)

**ç›®æ ‡**: 206ms â†’ 50ms (4.1xæå‡)

#### æ ¸å¿ƒæ€è·¯

å‡å°‘RPCè°ƒç”¨æ¬¡æ•°,å°†å¤šä¸ªGPUæ“ä½œåˆå¹¶ä¸ºä¸€æ¬¡RPC

#### å½“å‰é—®é¢˜

```python
# æ¯å±‚6æ¬¡RPCè°ƒç”¨
qkv = gpu.batch_linear(...)      # RPC 1
attn1 = gpu.matmul(Q, K.T)       # RPC 2
attn2 = gpu.matmul(attn, V)      # RPC 3
o = gpu.batch_linear(...)        # RPC 4
gate_up = gpu.batch_linear(...)  # RPC 5
down = gpu.batch_linear(...)     # RPC 6

# 16å±‚ Ã— 6æ¬¡ = 96æ¬¡
# + embedding(1æ¬¡) + lm_head(1æ¬¡) = 98æ¬¡
```

#### ä¼˜åŒ–æ–¹æ¡ˆ

```python
# æ–¹æ¡ˆA: æ¯å±‚1æ¬¡RPC (æ¿€è¿›)
output = gpu.fused_layer(input, layer_idx)  # åŒ…å«æ‰€æœ‰GPUæ“ä½œ

# æ–¹æ¡ˆB: æ¯å±‚2æ¬¡RPC (ä¿å®ˆ)
qkv_attn = gpu.fused_attention(input, layer_idx)  # QKV + Matmul
output = gpu.fused_mlp(attn_output, layer_idx)    # Gate/Up + Down
```

#### å®ç°æ­¥éª¤

1. **æœåŠ¡ç«¯æ·»åŠ èåˆç®—å­**:

```python
# server_optimized.py
@torch.no_grad()
def fused_attention(self, layer_idx: int, hidden_states: torch.Tensor) -> Dict:
    \"\"\"èåˆçš„Attentionæ“ä½œ\"\"\"
    layer = self.layers[layer_idx].self_attn
    
    # QKV projections
    q = layer.q_proj(hidden_states)
    k = layer.k_proj(hidden_states)
    v = layer.v_proj(hidden_states)
    
    # è¿”å›ç»™TEEåšRoPE + Softmax
    return {"q": q, "k": k, "v": v}

@torch.no_grad()
def fused_mlp(self, layer_idx: int, hidden_states: torch.Tensor, 
              gate_up: torch.Tensor) -> torch.Tensor:
    \"\"\"èåˆçš„MLPæ“ä½œ\"\"\"
    layer = self.layers[layer_idx].mlp
    
    # Down projection
    return layer.down_proj(gate_up)
```

2. **å®¢æˆ·ç«¯è°ƒç”¨èåˆç®—å­**:

```python
# tee_runner_optimized.py
def decoder_layer(self, layer_idx, hidden_states, position_ids):
    # TEE: Input norm
    hidden_states = self.input_layernorms[layer_idx](hidden_states)
    
    # GPU: Fused attention (1æ¬¡RPC)
    qkv = self.gpu.fused_attention(layer_idx, hidden_states)
    
    # TEE: RoPE + Softmax + Matmul
    attn_output = self._tee_attention(qkv, position_ids)
    
    # GPU: O projection (1æ¬¡RPC)
    attn_output = self.gpu.batch_linear(layer_idx, ["o_proj"], attn_output)[0]
    
    # ... MLPç±»ä¼¼
```

#### é¢„æœŸæ•ˆæœ

- RPCæ¬¡æ•°: 98 â†’ 18 (5.4x)
- RPCå¼€é”€: 50ms â†’ 10ms
- **æ€»å»¶è¿Ÿ**: 206ms â†’ 50ms

---

### é˜¶æ®µ 3: GPUä¼˜åŒ– (1å‘¨)

**ç›®æ ‡**: 50ms â†’ 10ms (5xæå‡)

#### ä¼˜åŒ–æ–¹å‘

1. **bfloat16ç²¾åº¦**
   - æ•°æ®é‡å‡åŠ: 7.5GB â†’ 3.75GB
   - GPUè®¡ç®—åŠ é€Ÿ: 1.5-2x
   
2. **Flash Attention**
   - å‡å°‘Matmulå¼€é”€
   - å†…å­˜æ•ˆç‡æå‡
   
3. **Kernelèåˆ**
   - å‡å°‘GPU kernelå¯åŠ¨å¼€é”€
   - æå‡å†…å­˜å¸¦å®½åˆ©ç”¨ç‡

#### å®ç°æ–¹æ¡ˆ

```python
# 1. bfloat16
model = model.to(torch.bfloat16)

# 2. Flash Attention
from flash_attn import flash_attn_func

attn_output = flash_attn_func(q, k, v, causal=True)

# 3. Kernelèåˆ
# ä½¿ç”¨torch.compileæˆ–æ‰‹å†™CUDA kernel
```

#### é¢„æœŸæ•ˆæœ

- GPUè®¡ç®—: 100ms â†’ 20ms (5x)
- æ•°æ®ä¼ è¾“: å‡åŠ
- **æ€»å»¶è¿Ÿ**: 50ms â†’ 10ms

---

## ğŸ“ˆ æ€§èƒ½é¢„æµ‹æ€»ç»“

| é˜¶æ®µ | ä¼˜åŒ– | å»¶è¿Ÿ | æå‡ | éš¾åº¦ | æ—¶é—´ |
|------|------|------|------|------|------|
| å½“å‰ | - | 332ms | 1x | - | - |
| é˜¶æ®µ1 | å…±äº«å†…å­˜ | 206ms | 1.6x | ä¸­ | 1-2å¤© |
| é˜¶æ®µ2 | ç®—å­èåˆ | 50ms | 6.6x | ä¸­ | 2-3å¤© |
| é˜¶æ®µ3 | GPUä¼˜åŒ– | 10ms | 33x | é«˜ | 1å‘¨ |

---

## ğŸš€ ç«‹å³è¡ŒåŠ¨

### ä¼˜å…ˆçº§1: å…±äº«å†…å­˜POC (ä»Šå¤©)

1. **å®‰è£…ä¾èµ–**:
```bash
pip install posix_ipc
```

2. **åˆ›å»ºæµ‹è¯•è„šæœ¬** `test_shared_memory.py`:
```python
import posix_ipc
import mmap
import numpy as np
import time

# åˆ›å»ºå…±äº«å†…å­˜
shm = posix_ipc.SharedMemory("/test_shm", posix_ipc.O_CREAT, size=100*1024*1024)
mem = mmap.mmap(shm.fd, shm.size)

# æµ‹è¯•å†™å…¥
data = np.random.rand(1024, 2048).astype(np.float32)
t0 = time.perf_counter()
mem[0:data.nbytes] = data.tobytes()
write_time = time.perf_counter() - t0

# æµ‹è¯•è¯»å–
t0 = time.perf_counter()
data2 = np.frombuffer(mem[0:data.nbytes], dtype=np.float32).reshape(1024, 2048)
read_time = time.perf_counter() - t0

print(f"å†™å…¥: {write_time*1000:.2f}ms")
print(f"è¯»å–: {read_time*1000:.2f}ms")
print(f"æ•°æ®é‡: {data.nbytes/1024/1024:.2f}MB")

# æ¸…ç†
shm.unlink()
```

3. **è¿è¡Œæµ‹è¯•**:
```bash
python test_shared_memory.py
```

**é¢„æœŸç»“æœ**: å†™å…¥/è¯»å–æ—¶é—´ < 1ms (è¿œå¿«äºmsgpackçš„7.5ms)

### ä¼˜å…ˆçº§2: å®ç°å…±äº«å†…å­˜ä¼ è¾“ (æ˜å¤©)

ä¿®æ”¹ `GPUClient` å’Œ `GPUServer` ä½¿ç”¨å…±äº«å†…å­˜

### ä¼˜å…ˆçº§3: æ€§èƒ½éªŒè¯ (åå¤©)

è¿è¡Œå®Œæ•´æ¨ç†,éªŒè¯æ€§èƒ½æå‡

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [POSIXå…±äº«å†…å­˜](https://docs.python.org/3/library/multiprocessing.shared_memory.html)
- [ZeroMQæ€§èƒ½ä¼˜åŒ–](https://zeromq.org/socket-api/)
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [PyTorchæ€§èƒ½ä¼˜åŒ–](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
